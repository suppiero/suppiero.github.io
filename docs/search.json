[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Piero Trujillo",
    "section": "",
    "text": "About Me\nHello üëãüèΩ My name is Piero Trujillo and welcome to my personal website!\nI recently graduated from the University of California, Santa Barbara, where I received a Bachelor‚Äôs Degree in Statistics & Data Science.\nI possess extensive knowledge in data science libraries using Python, tidymodels and tidyverse in R, as well as intermediate proficiency in SQL and PySpark for database and big data processing.\nIn my spare time, you can find me playing soccer, getting my hands dirty in ceramics, whipping up culinary delights in the kitchen, or being a foodie with my friends downtown.\n\n\nEducation\nBachelor‚Äôs in Statistics & Data Science | 2023 | UCSB Google Data Analytics Professional Certificate | 2023 Associate‚Äôs in Computer Science | 2021 | SBCC"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Portfolio",
    "section": "",
    "text": "Check out some of my favorite projects I worked on as an undergraduate student at UC Santa Barbara!\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpotify Classification Dashboard and Model Analysis\n\n\nCrafting predicitve models to determine whose playlist a song belongs to and summarizing results into an interactive dashboard.\n\n\n\nPiero Trujillo\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding and Modeling Human Mobility Response to California Wildfires\n\n\nA research poster created for the 2023 UCSB Data Science Capstone Showcase.\n\n\n\nPiero Trujillo, Justin Liu, Lyndsey Umsted, Ellen Burrell\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Points in Fantasy Premier League\n\n\nUsing machine learning to propel myself to the top of my mini league!\n\n\n\nPiero Trujillo\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Legendary Pokemon\n\n\nPredicting whether a pokemon is legendary based on their stats & attributes.\n\n\n\nPiero Trujillo and Randy Ross\n\n\nDec 4, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#links-poster",
    "href": "projects.html#links-poster",
    "title": "Projects",
    "section": "Links: Poster",
    "text": "Links: Poster"
  },
  {
    "objectID": "projects/MOVE_Lab/index.html#project-background",
    "href": "projects/MOVE_Lab/index.html#project-background",
    "title": "Understanding and Modeling Human Mobility Response to California Wildfires",
    "section": "",
    "text": "In this project, I collaborated with a team of four students on a six-month research project sponsored by the Movement Data Science Lab at UCSB where we utilized time series datasets tracking human mobility to analyze changes in movement patterns during California wildfires. Specifically, we focused on the Lake Fire in Los Angeles County, which occurred from August to September in 2020. In this project, we used spatial-temporal data science techniques and machine learning applications to model human mobility in response to wildfires. Our main objective was to apply machine learning techniques to identify and trace changes in mobility time series of wildfire events. The project‚Äôs main finding is that mobility in locations closer to the fire‚Äôs edge and locations in the direction of the fire‚Äôs burn experience a greater impact from wildfires. Similarly, the categories of locations experiencing a significant impact in mobility from wildfires are Historical/Nature, Public Functions, Grocery, Gasoline, Religious, and Childcare. Furthermore, we produced a number of presentations, including a poster presentation for the UC Santa Barbara Data Science Capstone Project Showcase."
  },
  {
    "objectID": "projects/MOVE_Lab/index.html#poster",
    "href": "projects/MOVE_Lab/index.html#poster",
    "title": "Understanding and Modeling Human Mobility Response to California Wildfires",
    "section": "Poster",
    "text": "Poster\nBelow you can find our project poster that we presented at the UCSB Data Science Capstone Showcase!"
  },
  {
    "objectID": "projects/fantasy_premier_league/index.html",
    "href": "projects/fantasy_premier_league/index.html",
    "title": "Predicting Points in Fantasy Premier League",
    "section": "",
    "text": "This machine learning project leverages gameweek-specific data from Fantasy Premier League, considering fundamental player statistics such as bonus points, expected points, goals scored, assists, ict_index, influence, threat, and creativity. Its objective is to develop a model that accurately predicts a player‚Äôs actual weekly point total in that specific week of the Fantasy Premier League. The six models I developed were Linear Regression, Decision Tree, Random Forest, Boosted Trees, Lasso Regression, and Ridge Regression. Upon thorough evaluation using three key metrics: RMSE, MAE, and R-Squared, the Random Forest model emerged as the greatest model, distinguished by its commendable accuracy yielding an RMSE value of 0.519.\nLinks: Project Report and GitHub repo\n\n\n\nCitationBibTeX citation:@online{trujillo2022,\n  author = {Trujillo, Piero},\n  title = {Predicting {Points} in {Fantasy} {Premier} {League}},\n  date = {2022-12-11},\n  url = {https://suppiero.github.io/projects/fantasy_premier_league/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTrujillo, Piero. 2022. ‚ÄúPredicting Points in Fantasy Premier\nLeague.‚Äù December 11, 2022. https://suppiero.github.io/projects/fantasy_premier_league/."
  },
  {
    "objectID": "projects/pokemon_legendary_classifier/index.html",
    "href": "projects/pokemon_legendary_classifier/index.html",
    "title": "Classifying Legendary Pokemon",
    "section": "",
    "text": "The focus of our project was to find a reliable way to classify legendary pokemon based on their different attributes. Many attributes of a pokemon were taken into account. These included their capture rate as well as their attack, defense, and speed stats. With our findings we could compare our results with ordinary pokemon, whose stats neighbored those of a legendary pokemon, in order to strong pokemon that would ordinarily be overlooked. Our data showed that the best classifiers for a legendary pokemon were their capture rate and defense stat. Therefore, legendary pokemon must have low capture rates and high defense stats. With this in mind, we were able to classify legendary pokemon with a success rate of 95.9%."
  },
  {
    "objectID": "projects/MOVE_Lab/index.html",
    "href": "projects/MOVE_Lab/index.html",
    "title": "Understanding and Modeling Human Mobility Response to California Wildfires",
    "section": "",
    "text": "In this project, I collaborated with a team of four students on a six-month research project sponsored by the Movement Data Science Lab at UCSB where we utilized time series datasets tracking human mobility to analyze changes in movement patterns during California wildfires. Specifically, we focused on the Lake Fire in Los Angeles County, which occurred from August to September in 2020. In this project, we used spatial-temporal data science techniques and machine learning applications to model human mobility in response to wildfires. Our main objective was to apply machine learning techniques to identify and trace changes in mobility time series of wildfire events. The project‚Äôs main finding is that mobility in locations closer to the fire‚Äôs edge and locations in the direction of the fire‚Äôs burn experience a greater impact from wildfires. Similarly, the categories of locations experiencing a significant impact in mobility from wildfires are Historical/Nature, Public Functions, Grocery, Gasoline, Religious, and Childcare. Furthermore, we produced a number of presentations, including a poster presentation for the UC Santa Barbara Data Science Capstone Project Showcase."
  },
  {
    "objectID": "projects/pokemon_legendary_classifier/project.html#the-complete-pokemon-dataset---rounak-banik",
    "href": "projects/pokemon_legendary_classifier/project.html#the-complete-pokemon-dataset---rounak-banik",
    "title": "Legendary Pokemon Classifier",
    "section": "The Complete Pokemon Dataset - Rounak Banik",
    "text": "The Complete Pokemon Dataset - Rounak Banik\nhttps://www.kaggle.com/rounakbanik/pokemon\nEthical Considerations\n\nRounak Banik collected this data from http://serebii.net/, a very reputable Pokemon fan-site, because Pokemon is very special to him and built the dataset out of passion and curiosity for the game.\nWe believe the analysis of this dataset does not cause harm to anyone because those represented in the data only exist virtually.\nDigital beings known as Pokemon are represented in the dataset. Nothing is being over-represented because the data is focusing solely on Pokemon and not any other groups.\nThis could be considered ethically wrong if you are new to the game and want to experience it naturally.\n\nRelevant Attributes - The only data missing from the dataset is Pokemon generation 8 (released in 2019), which would bring it up to date. - We are attempting to classify whether or not a certain Pokemon is legendary or not. + The variable will be either 0 or 1, where 0 means not legendary, and 1 means legendary. - We will use attributes attack and sp_attack, defense and sp_defense (sp = special), capture rate, hp (hit points), and speed. - We removed one Pokemon whose capture rate was ‚Äú30 (Meteorite)255 (Core)‚Äù, which is not a plain number."
  },
  {
    "objectID": "projects/pokemon_legendary_classifier/project.html#eda",
    "href": "projects/pokemon_legendary_classifier/project.html#eda",
    "title": "Legendary Pokemon Classifier",
    "section": "EDA",
    "text": "EDA\n\npokemon.scatter(\"attack\", \"defense\", group=\"is_legendary\")\n\n\n\n\n\npokemon.scatter(\"sp_attack\", \"sp_defense\", group=\"is_legendary\")\n\n\n\n\n\npokemon.scatter(\"hp\", \"defense\", group=\"is_legendary\")\n\n\n\n\n\nax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')\nax.scatter(pokemon.column(\"attack\"),\n           pokemon.column(\"defense\"),\n           pokemon.column(\"speed\"),\n           c=pokemon.column(\"is_legendary\"));\n\n\n\n\n\nax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')\nax.scatter(pokemon.column(\"capture_rate\"), \n           pokemon.column(\"attack\"), \n           pokemon.column(\"defense\"), \n           c=pokemon.column(\"is_legendary\"));\n\n\n\n\n\nbest_columns = pokemon.select(\"name\", \"attack\", \"defense\", \"capture_rate\", \"is_legendary\")\n\nlegendaries = best_columns.where(\"is_legendary\", are.equal_to(1))\n\ncapture_rate = legendaries.group(\"capture_rate\")\nattack = legendaries.group(\"attack\")\ndefense = legendaries.group(\"defense\")\n\ncapture_rate.show()\n# attack.show()\n# defense.show()\n\n\n\n\ncapture_rate\ncount\n\n\n\n\n3\n53\n\n\n15\n1\n\n\n25\n2\n\n\n30\n1\n\n\n45\n11\n\n\n255\n2\n\n\n\n\n\nThe above table shows that 53 of the 70 legendary Pokemon have the lowest capture rate in the game.\n\nlowest_capture_rate = best_columns.sort(\"capture_rate\")\nlowest_capture_rate.take(range(20)).show()\n\n\n\n\nname\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nArticuno\n85\n100\n3\n1\n\n\nZapdos\n90\n85\n3\n1\n\n\nMoltres\n100\n90\n3\n1\n\n\nMewtwo\n150\n70\n3\n1\n\n\nRaikou\n85\n75\n3\n1\n\n\nEntei\n115\n85\n3\n1\n\n\nSuicune\n75\n115\n3\n1\n\n\nLugia\n90\n130\n3\n1\n\n\nHo-Oh\n130\n90\n3\n1\n\n\nBeldum\n55\n80\n3\n0\n\n\nMetang\n75\n100\n3\n0\n\n\nMetagross\n145\n150\n3\n0\n\n\nRegirock\n100\n200\n3\n1\n\n\nRegice\n50\n100\n3\n1\n\n\nRegisteel\n75\n150\n3\n1\n\n\nLatias\n100\n120\n3\n1\n\n\nLatios\n130\n100\n3\n1\n\n\nKyogre\n150\n90\n3\n1\n\n\nGroudon\n180\n160\n3\n1\n\n\nJirachi\n100\n100\n3\n1\n\n\n\n\n\nThe above table shows that only 3 of the 20 lowest capture rates are non-legendary.\n\nhighest_defense = best_columns.sort(\"defense\", descending=True)\n\nhighest_defense_20 = highest_defense.take(range(20))\nhighest_defense_20.show()\nhighest_defense_20.group(\"is_legendary\")\n\n\n\n\nname\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nSteelix\n125\n230\n25\n0\n\n\nShuckle\n10\n230\n190\n0\n\n\nAggron\n140\n230\n45\n0\n\n\nRegirock\n100\n200\n3\n1\n\n\nAvalugg\n117\n184\n55\n0\n\n\nSlowbro\n75\n180\n75\n0\n\n\nCloyster\n95\n180\n60\n0\n\n\nBastiodon\n52\n168\n45\n0\n\n\nOnix\n45\n160\n45\n0\n\n\nGroudon\n180\n160\n3\n1\n\n\nToxapex\n63\n152\n75\n0\n\n\nTyranitar\n164\n150\n45\n0\n\n\nMetagross\n145\n150\n3\n0\n\n\nRegisteel\n75\n150\n3\n1\n\n\nDoublade\n110\n150\n90\n0\n\n\nCarbink\n50\n150\n60\n0\n\n\nProbopass\n55\n145\n60\n0\n\n\nCofagrigus\n50\n145\n90\n0\n\n\nForretress\n90\n140\n75\n0\n\n\nScizor\n150\n140\n25\n0\n\n\n\n\n\n\n\n\nis_legendary\ncount\n\n\n\n\n0\n17\n\n\n1\n3\n\n\n\n\n\n\nhighest_attack = best_columns.sort(\"attack\", descending=True)\n\nhighest_attack_20 = highest_attack.take(range(20))\nhighest_attack_100 = highest_attack.take(range(100))\nhighest_attack_200 = highest_attack.take(range(200))\n\n# Number of legendaries in top defense pokemon\nhighest_attack_20_sum = highest_attack_20.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\nhighest_attack_100_sum = highest_attack_100.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\nhighest_attack_200_sum = highest_attack_200.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\n\nprint(\"% legendary in top 20:\", (highest_attack_20_sum / 20) * 100)\nprint(\"% legendary in top 100:\", (highest_attack_100_sum / 100) * 100)\nprint(\"% legendary in top 200:\", (highest_attack_200_sum / 200) * 100)\n\n% legendary in top 20: 35.0\n% legendary in top 100: 28.000000000000004\n% legendary in top 200: 23.0\n\n\nThis may be implying that when sorted by attack, more legendaries are concentrated at the top.\n\nhighest_defense_20 = highest_defense.take(range(20))\nhighest_defense_100 = highest_defense.take(range(100))\nhighest_defense_200 = highest_defense.take(range(200))\n\n# Number of legendaries in top defense pokemon\nhighest_defense_20_sum = highest_defense_20.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\nhighest_defense_100_sum = highest_defense_100.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\nhighest_defense_200_sum = highest_defense_200.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\n\nprint(\"% legendary in top 20:\", (highest_defense_20_sum / 20) * 100)\nprint(\"% legendary in top 100:\", (highest_defense_100_sum / 100) * 100)\nprint(\"% legendary in top 200:\", (highest_defense_200_sum / 200) * 100)\n\n% legendary in top 20: 15.0\n% legendary in top 100: 22.0\n% legendary in top 200: 23.5\n\n\nUnlike attack, when sorted by defense, there is not a higher concentration of legendaries at the top.\n\nlowest_defense = best_columns.sort(\"defense\")\n\nlowest_defense_20 = lowest_defense.take(range(20))\nlowest_defense_20.show()\nlowest_defense_20.group(\"is_legendary\")\n\n\n\n\nname\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nChansey\n5\n5\n30\n0\n\n\nHappiny\n5\n5\n130\n0\n\n\nBlissey\n10\n10\n30\n0\n\n\nAbra\n20\n15\n200\n0\n\n\nPichu\n40\n15\n190\n0\n\n\nIgglybuff\n30\n15\n170\n0\n\n\nSmoochum\n30\n15\n45\n0\n\n\nJigglypuff\n45\n20\n170\n0\n\n\nCarvanha\n90\n20\n225\n0\n\n\nFeebas\n15\n20\n255\n0\n\n\nWhismur\n51\n23\n190\n0\n\n\nRalts\n25\n25\n235\n0\n\n\nCleffa\n25\n28\n150\n0\n\n\nWeedle\n35\n30\n255\n0\n\n\nSpearow\n60\n30\n255\n0\n\n\nDiglett\n55\n30\n255\n0\n\n\nKadabra\n35\n30\n100\n0\n\n\nGastly\n35\n30\n190\n0\n\n\nHoothoot\n30\n30\n255\n0\n\n\nLedyba\n20\n30\n255\n0\n\n\n\n\n\n\n\n\nis_legendary\ncount\n\n\n\n\n0\n20\n\n\n\n\n\n\nlowest_attack = best_columns.sort(\"attack\")\n\nlowest_attack_20 = lowest_attack.take(range(20))\nlowest_attack_20.show()\nlowest_attack_20.group(\"is_legendary\")\n\n\n\n\nname\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nChansey\n5\n5\n30\n0\n\n\nHappiny\n5\n5\n130\n0\n\n\nMagikarp\n10\n55\n255\n0\n\n\nShuckle\n10\n230\n190\n0\n\n\nBlissey\n10\n10\n30\n0\n\n\nFeebas\n15\n20\n255\n0\n\n\nMetapod\n20\n55\n120\n0\n\n\nAbra\n20\n15\n200\n0\n\n\nLedyba\n20\n30\n255\n0\n\n\nTogepi\n20\n65\n190\n0\n\n\nMarill\n20\n50\n190\n0\n\n\nSmeargle\n20\n35\n45\n0\n\n\nAzurill\n20\n40\n150\n0\n\n\nMantyke\n20\n50\n25\n0\n\n\nSpewpa\n22\n60\n120\n0\n\n\nWynaut\n23\n48\n125\n0\n\n\nBronzor\n24\n86\n255\n0\n\n\nKakuna\n25\n50\n120\n0\n\n\nCleffa\n25\n28\n150\n0\n\n\nRalts\n25\n25\n235\n0\n\n\n\n\n\n\n\n\nis_legendary\ncount\n\n\n\n\n0\n20\n\n\n\n\n\nIt seems that the capture rate of the Pokemon is the best indicator of whether it is legendary or not. The attack and defense of the Pokemon may also be a good indicator."
  },
  {
    "objectID": "projects/pokemon_legendary_classifier/project.html#training-and-testing-sets",
    "href": "projects/pokemon_legendary_classifier/project.html#training-and-testing-sets",
    "title": "Legendary Pokemon Classifier",
    "section": "Training and testing sets",
    "text": "Training and testing sets\n\npokemon_classify_columns = pokemon.select(\"attack\", \"defense\", \"capture_rate\", \"is_legendary\")\n\npokemon_classify_columns.stats()\n\n\n\n\nstatistic\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nmin\n5\n5\n3\n0\n\n\nmax\n185\n230\n255\n1\n\n\nmedian\n75\n70\n60\n0\n\n\nsum\n62264\n58420\n79009\n70\n\n\n\n\n\n\nall_legendaries = pokemon_classify_columns.where(\"is_legendary\", are.equal_to(1)).sample(with_replacement=False)\nnon_legendaries = pokemon_classify_columns.where(\"is_legendary\", are.equal_to(0)).sample(with_replacement=False)\n\nnum_legendary = all_legendaries.num_rows\nnum_non_legendary = non_legendaries.num_rows\n\nnum_legendary_training = int(0.667 * num_legendary)\nnum_non_legendary_training = int(0.667 * num_non_legendary)\n\n\n# Take first two thirds legendary and non-legendary pokemon\ntraining_legendary = all_legendaries.take(np.arange(num_legendary_training))\ntraining_non_legendary = non_legendaries.take(np.arange(num_non_legendary_training))\n\n\n# Take last third legendary and non-legendary\ntesting_legendary = all_legendaries.take(np.arange(num_legendary_training, num_legendary))\ntesting_non_legendary = non_legendaries.take(np.arange(num_non_legendary_training, num_non_legendary))\n\n\n# Append training and testing legendary and non-legendary\ntesting_legendary.append(testing_non_legendary)\ntraining_legendary.append(training_non_legendary)\n\n\n# Shuffle training and testing and assign new names\ntraining = training_legendary.shuffle()\ntesting = testing_legendary.shuffle()\n\ntraining.relabel(\"is_legendary\", \"Class\")\ntesting.relabel(\"is_legendary\", \"Class\")\n\n\n\n\nattack\ndefense\ncapture_rate\nClass\n\n\n\n\n92\n108\n100\n0\n\n\n50\n120\n75\n0\n\n\n135\n105\n75\n0\n\n\n15\n20\n255\n0\n\n\n95\n95\n45\n0\n\n\n105\n90\n45\n0\n\n\n115\n60\n60\n0\n\n\n74\n74\n45\n0\n\n\n120\n75\n45\n0\n\n\n47\n75\n150\n0\n\n\n\n\n... (258 rows omitted)\n\n\nWe are putting two thirds of all the Pokemon into the training set, and the other third into the testing set. To make sure both sets have legendary Pokemon, first the legendary pokemon are separated from the non-legendary, and two thirds from each catagory are put into the training set, and the rest into the testing set.\n\nClassifier\n\ndef accuracy(predictions, labels):\n    diff = labels - predictions\n    num_incorrect = np.count_nonzero(diff)\n    num_correct = len(labels) - num_incorrect\n    accuracy = num_correct / len(labels)\n    return accuracy\n\n\ndef distance_nn(point1, point2):\n    \"\"\"The distance between two arrays of numbers.\"\"\"\n    return np.sqrt(np.sum((point1 - point2)**2))\n\ndef all_distances(training, point):\n    \"\"\"The distance between p (an array of numbers) and the numbers in row i of attribute_table.\"\"\"\n    attributes = training.drop('Class')\n    def distance_from_point(row):\n        return distance_nn(point, np.array(row))\n    return attributes.apply(distance_from_point)\n\ndef table_with_distances(training, point):\n    \"\"\"A copy of the training table with the distance from each row to array p.\"\"\"\n    return training.with_column('Distance', all_distances(training, point))\n\ndef closest(training, point, k):\n    \"\"\"A table containing the k closest rows in the training table to array p.\"\"\"\n    with_dists = table_with_distances(training, point)\n    sorted_by_distance = with_dists.sort('Distance')\n    topk = sorted_by_distance.take(np.arange(k))\n    return topk\n\ndef majority(topkclasses):\n    \"\"\"1 if the majority of the \"Class\" column is 1s, and 0 otherwise.\"\"\"\n    ones = topkclasses.where('Class', are.equal_to(1)).num_rows\n    zeros = topkclasses.where('Class', are.equal_to(0)).num_rows\n    if ones &gt; zeros:\n        return 1\n    else:\n        return 0\n\ndef classify(training, p, k):\n    \"\"\"Classify an example with attributes p using k-nearest neighbor classification with the given training table.\"\"\"\n    closestk = closest(training, p, k)\n    topkclasses = closestk.select('Class')\n    return majority(topkclasses)\n\ndef classify_table(training, points, k):\n    \"\"\"Classify a table of unlabled points using KNN\"\"\"\n    def classify_p(p):\n        return classify(training, p, k)\n\n    classes = points.apply(classify_p)\n    return points.with_column('Class', classes)"
  },
  {
    "objectID": "projects/pokemon_legendary_classifier/project.html#classifying-legendary-pokemon",
    "href": "projects/pokemon_legendary_classifier/project.html#classifying-legendary-pokemon",
    "title": "Legendary Pokemon Classifier",
    "section": "Classifying Legendary Pokemon",
    "text": "Classifying Legendary Pokemon\n\nprediction_1 = classify_table(training, testing.drop(\"Class\"), 1)\nprediction_3 = classify_table(training, testing.drop(\"Class\"), 3)\nprediction_5 = classify_table(training, testing.drop(\"Class\"), 5)\n\n\naccuracy_1 = accuracy(prediction_1.column(\"Class\"), testing.column(\"Class\"))\naccuracy_3 = accuracy(prediction_3.column(\"Class\"), testing.column(\"Class\"))\naccuracy_5 = accuracy(prediction_5.column(\"Class\"), testing.column(\"Class\"))\n\nprint(\"k=1 -&gt;\", accuracy_1)\nprint(\"k=3 -&gt;\", accuracy_3)\nprint(\"k=5 -&gt;\", accuracy_5)\n\n\nResults\nThe classifier was run using a few different k-values for the k-nearest neighbor algorithm. Using the 1st nearest-neighbor, the accuracy was about 95%. Going to the 3 nearest neighbors, the accuracy went up to about 95.5%. With 5 nearest neighbors, the accuracy was 95.9%. Increasing the k-value just slightly increases the accuracy of the classifier."
  },
  {
    "objectID": "projects/pokemon_legendary_classifier/index.html#the-complete-pokemon-dataset---rounak-banik",
    "href": "projects/pokemon_legendary_classifier/index.html#the-complete-pokemon-dataset---rounak-banik",
    "title": "Classifying Legendary Pokemon",
    "section": "The Complete Pokemon Dataset - Rounak Banik",
    "text": "The Complete Pokemon Dataset - Rounak Banik\nhttps://www.kaggle.com/rounakbanik/pokemon\nEthical Considerations\n\nRounak Banik collected this data from http://serebii.net/, a very reputable Pokemon fan-site, because Pokemon is very special to him and built the dataset out of passion and curiosity for the game.\nWe believe the analysis of this dataset does not cause harm to anyone because those represented in the data only exist virtually.\nDigital beings known as Pokemon are represented in the dataset. Nothing is being over-represented because the data is focusing solely on Pokemon and not any other groups.\nThis could be considered ethically wrong if you are new to the game and want to experience it naturally.\n\nRelevant Attributes - The only data missing from the dataset is Pokemon generation 8 (released in 2019), which would bring it up to date. - We are attempting to classify whether or not a certain Pokemon is legendary or not. + The variable will be either 0 or 1, where 0 means not legendary, and 1 means legendary. - We will use attributes attack and sp_attack, defense and sp_defense (sp = special), capture rate, hp (hit points), and speed. - We removed one Pokemon whose capture rate was ‚Äú30 (Meteorite)255 (Core)‚Äù, which is not a plain number."
  },
  {
    "objectID": "projects/pokemon_legendary_classifier/index.html#eda",
    "href": "projects/pokemon_legendary_classifier/index.html#eda",
    "title": "Classifying Legendary Pokemon",
    "section": "EDA",
    "text": "EDA\n\npokemon.scatter(\"attack\", \"defense\", group=\"is_legendary\")\n\n\n\n\n\npokemon.scatter(\"sp_attack\", \"sp_defense\", group=\"is_legendary\")\n\n\n\n\n\npokemon.scatter(\"hp\", \"defense\", group=\"is_legendary\")\n\n\n\n\n\nax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')\nax.scatter(pokemon.column(\"attack\"),\n        pokemon.column(\"defense\"),\n        pokemon.column(\"speed\"),\n        c=pokemon.column(\"is_legendary\"));\n\n\n\n\n\nax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')\nax.scatter(pokemon.column(\"capture_rate\"),\n        pokemon.column(\"attack\"),\n        pokemon.column(\"defense\"),\n        c=pokemon.column(\"is_legendary\"));\n\n\n\n\n\nbest_columns = pokemon.select(\"name\", \"attack\", \"defense\", \"capture_rate\", \"is_legendary\")\n\nlegendaries = best_columns.where(\"is_legendary\", are.equal_to(1))\n\ncapture_rate = legendaries.group(\"capture_rate\")\nattack = legendaries.group(\"attack\")\ndefense = legendaries.group(\"defense\")\n\ncapture_rate.show()\n# attack.show()\n# defense.show()\n\n\n\n\ncapture_rate\ncount\n\n\n\n\n3\n53\n\n\n15\n1\n\n\n25\n2\n\n\n30\n1\n\n\n45\n11\n\n\n255\n2\n\n\n\n\n\nThe above table shows that 53 of the 70 legendary Pokemon have the lowest capture rate in the game.\n\nlowest_capture_rate = best_columns.sort(\"capture_rate\")\nlowest_capture_rate.take(range(20)).show()\n\n\n\n\nname\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nArticuno\n85\n100\n3\n1\n\n\nZapdos\n90\n85\n3\n1\n\n\nMoltres\n100\n90\n3\n1\n\n\nMewtwo\n150\n70\n3\n1\n\n\nRaikou\n85\n75\n3\n1\n\n\nEntei\n115\n85\n3\n1\n\n\nSuicune\n75\n115\n3\n1\n\n\nLugia\n90\n130\n3\n1\n\n\nHo-Oh\n130\n90\n3\n1\n\n\nBeldum\n55\n80\n3\n0\n\n\nMetang\n75\n100\n3\n0\n\n\nMetagross\n145\n150\n3\n0\n\n\nRegirock\n100\n200\n3\n1\n\n\nRegice\n50\n100\n3\n1\n\n\nRegisteel\n75\n150\n3\n1\n\n\nLatias\n100\n120\n3\n1\n\n\nLatios\n130\n100\n3\n1\n\n\nKyogre\n150\n90\n3\n1\n\n\nGroudon\n180\n160\n3\n1\n\n\nJirachi\n100\n100\n3\n1\n\n\n\n\n\nThe above table shows that only 3 of the 20 lowest capture rates are non-legendary.\n\nhighest_defense = best_columns.sort(\"defense\", descending=True)\n\nhighest_defense_20 = highest_defense.take(range(20))\nhighest_defense_20.show()\nhighest_defense_20.group(\"is_legendary\")\n\n\n\n\nname\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nSteelix\n125\n230\n25\n0\n\n\nShuckle\n10\n230\n190\n0\n\n\nAggron\n140\n230\n45\n0\n\n\nRegirock\n100\n200\n3\n1\n\n\nAvalugg\n117\n184\n55\n0\n\n\nSlowbro\n75\n180\n75\n0\n\n\nCloyster\n95\n180\n60\n0\n\n\nBastiodon\n52\n168\n45\n0\n\n\nOnix\n45\n160\n45\n0\n\n\nGroudon\n180\n160\n3\n1\n\n\nToxapex\n63\n152\n75\n0\n\n\nTyranitar\n164\n150\n45\n0\n\n\nMetagross\n145\n150\n3\n0\n\n\nRegisteel\n75\n150\n3\n1\n\n\nDoublade\n110\n150\n90\n0\n\n\nCarbink\n50\n150\n60\n0\n\n\nProbopass\n55\n145\n60\n0\n\n\nCofagrigus\n50\n145\n90\n0\n\n\nForretress\n90\n140\n75\n0\n\n\nScizor\n150\n140\n25\n0\n\n\n\n\n\n\n\n\nis_legendary\ncount\n\n\n\n\n0\n17\n\n\n1\n3\n\n\n\n\n\n\nhighest_attack = best_columns.sort(\"attack\", descending=True)\n\nhighest_attack_20 = highest_attack.take(range(20))\nhighest_attack_100 = highest_attack.take(range(100))\nhighest_attack_200 = highest_attack.take(range(200))\n\n# Number of legendaries in top defense pokemon\nhighest_attack_20_sum = highest_attack_20.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\nhighest_attack_100_sum = highest_attack_100.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\nhighest_attack_200_sum = highest_attack_200.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\n\nprint(\"% legendary in top 20:\", (highest_attack_20_sum / 20) * 100)\nprint(\"% legendary in top 100:\", (highest_attack_100_sum / 100) * 100)\nprint(\"% legendary in top 200:\", (highest_attack_200_sum / 200) * 100)\n\n% legendary in top 20: 35.0\n% legendary in top 100: 28.0\n% legendary in top 200: 23.0\n\n\nThis may be implying that when sorted by attack, more legendaries are concentrated at the top.\n\nhighest_defense_20 = highest_defense.take(range(20))\nhighest_defense_100 = highest_defense.take(range(100))\nhighest_defense_200 = highest_defense.take(range(200))\n\n# Number of legendaries in top defense pokemon\nhighest_defense_20_sum = highest_defense_20.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\nhighest_defense_100_sum = highest_defense_100.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\nhighest_defense_200_sum = highest_defense_200.stats().where(\"statistic\", are.equal_to(\"sum\"))[5][0]\n\nprint(\"% legendary in top 20:\", (highest_defense_20_sum / 20) * 100)\nprint(\"% legendary in top 100:\", (highest_defense_100_sum / 100) * 100)\nprint(\"% legendary in top 200:\", (highest_defense_200_sum / 200) * 100)\n\n% legendary in top 20: 15.0\n% legendary in top 100: 22.0\n% legendary in top 200: 23.5\n\n\nUnlike attack, when sorted by defense, there is not a higher concentration of legendaries at the top.\n\nlowest_defense = best_columns.sort(\"defense\")\n\nlowest_defense_20 = lowest_defense.take(range(20))\nlowest_defense_20.show()\nlowest_defense_20.group(\"is_legendary\")\n\n\n\n\nname\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nChansey\n5\n5\n30\n0\n\n\nHappiny\n5\n5\n130\n0\n\n\nBlissey\n10\n10\n30\n0\n\n\nAbra\n20\n15\n200\n0\n\n\nPichu\n40\n15\n190\n0\n\n\nIgglybuff\n30\n15\n170\n0\n\n\nSmoochum\n30\n15\n45\n0\n\n\nJigglypuff\n45\n20\n170\n0\n\n\nCarvanha\n90\n20\n225\n0\n\n\nFeebas\n15\n20\n255\n0\n\n\nWhismur\n51\n23\n190\n0\n\n\nRalts\n25\n25\n235\n0\n\n\nCleffa\n25\n28\n150\n0\n\n\nWeedle\n35\n30\n255\n0\n\n\nSpearow\n60\n30\n255\n0\n\n\nDiglett\n55\n30\n255\n0\n\n\nKadabra\n35\n30\n100\n0\n\n\nGastly\n35\n30\n190\n0\n\n\nHoothoot\n30\n30\n255\n0\n\n\nLedyba\n20\n30\n255\n0\n\n\n\n\n\n\n\n\nis_legendary\ncount\n\n\n\n\n0\n20\n\n\n\n\n\n\nlowest_attack = best_columns.sort(\"attack\")\n\nlowest_attack_20 = lowest_attack.take(range(20))\nlowest_attack_20.show()\nlowest_attack_20.group(\"is_legendary\")\n\n\n\n\nname\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nChansey\n5\n5\n30\n0\n\n\nHappiny\n5\n5\n130\n0\n\n\nMagikarp\n10\n55\n255\n0\n\n\nShuckle\n10\n230\n190\n0\n\n\nBlissey\n10\n10\n30\n0\n\n\nFeebas\n15\n20\n255\n0\n\n\nMetapod\n20\n55\n120\n0\n\n\nAbra\n20\n15\n200\n0\n\n\nLedyba\n20\n30\n255\n0\n\n\nTogepi\n20\n65\n190\n0\n\n\nMarill\n20\n50\n190\n0\n\n\nSmeargle\n20\n35\n45\n0\n\n\nAzurill\n20\n40\n150\n0\n\n\nMantyke\n20\n50\n25\n0\n\n\nSpewpa\n22\n60\n120\n0\n\n\nWynaut\n23\n48\n125\n0\n\n\nBronzor\n24\n86\n255\n0\n\n\nKakuna\n25\n50\n120\n0\n\n\nCleffa\n25\n28\n150\n0\n\n\nRalts\n25\n25\n235\n0\n\n\n\n\n\n\n\n\nis_legendary\ncount\n\n\n\n\n0\n20\n\n\n\n\n\nIt seems that the capture rate of the Pokemon is the best indicator of whether it is legendary or not. The attack and defense of the Pokemon may also be a good indicator."
  },
  {
    "objectID": "projects/pokemon_legendary_classifier/index.html#training-and-testing-sets",
    "href": "projects/pokemon_legendary_classifier/index.html#training-and-testing-sets",
    "title": "Classifying Legendary Pokemon",
    "section": "Training and testing sets",
    "text": "Training and testing sets\n\npokemon_classify_columns = pokemon.select(\"attack\", \"defense\", \"capture_rate\", \"is_legendary\")\n\npokemon_classify_columns.stats()\n\n\n\n\nstatistic\nattack\ndefense\ncapture_rate\nis_legendary\n\n\n\n\nmin\n5\n5\n3\n0\n\n\nmax\n185\n230\n255\n1\n\n\nmedian\n75\n70\n60\n0\n\n\nsum\n62264\n58420\n79009\n70\n\n\n\n\n\n\nall_legendaries = pokemon_classify_columns.where(\"is_legendary\", are.equal_to(1)).sample(with_replacement=False)\nnon_legendaries = pokemon_classify_columns.where(\"is_legendary\", are.equal_to(0)).sample(with_replacement=False)\n\nnum_legendary = all_legendaries.num_rows\nnum_non_legendary = non_legendaries.num_rows\n\nnum_legendary_training = int(0.667 * num_legendary)\nnum_non_legendary_training = int(0.667 * num_non_legendary)\n\n\n# Take first two thirds legendary and non-legendary pokemon\ntraining_legendary = all_legendaries.take(np.arange(num_legendary_training))\ntraining_non_legendary = non_legendaries.take(np.arange(num_non_legendary_training))\n\n\n# Take last third legendary and non-legendary\ntesting_legendary = all_legendaries.take(np.arange(num_legendary_training, num_legendary))\ntesting_non_legendary = non_legendaries.take(np.arange(num_non_legendary_training, num_non_legendary))\n\n\n# Append training and testing legendary and non-legendary\ntesting_legendary.append(testing_non_legendary)\ntraining_legendary.append(training_non_legendary)\n\n\n# Shuffle training and testing and assign new names\ntraining = training_legendary.shuffle()\ntesting = testing_legendary.shuffle()\n\ntraining.relabel(\"is_legendary\", \"Class\")\ntesting.relabel(\"is_legendary\", \"Class\")\n\n\n\n\nattack\ndefense\ncapture_rate\nClass\n\n\n\n\n92\n108\n100\n0\n\n\n50\n120\n75\n0\n\n\n135\n105\n75\n0\n\n\n15\n20\n255\n0\n\n\n95\n95\n45\n0\n\n\n105\n90\n45\n0\n\n\n115\n60\n60\n0\n\n\n74\n74\n45\n0\n\n\n120\n75\n45\n0\n\n\n47\n75\n150\n0\n\n\n\n\n... (258 rows omitted)\n\n\nWe are putting two thirds of all the Pokemon into the training set, and the other third into the testing set. To make sure both sets have legendary Pokemon, first the legendary pokemon are separated from the non-legendary, and two thirds from each catagory are put into the training set, and the rest into the testing set.\n\nClassifier\n\ndef accuracy(predictions, labels):\n    diff = labels - predictions\n    num_incorrect = np.count_nonzero(diff)\n    num_correct = len(labels) - num_incorrect\n    accuracy = num_correct / len(labels)\n    return accuracy\n\n\ndef distance_nn(point1, point2):\n  \"\"\"The distance between two arrays of numbers.\"\"\"\n  return np.sqrt(np.sum((point1 - point2)**2))\n\ndef all_distances(training, point):\n  \"\"\"The distance between p (an array of numbers) and the numbers in row i of attribute_table.\"\"\"\n  attributes = training.drop('Class')\n  def distance_from_point(row):\n    return distance_nn(point, np.array(row))\n  return attributes.apply(distance_from_point)\n\ndef table_with_distances(training, point):\n  \"\"\"A copy of the training table with the distance from each row to array p.\"\"\"\n  return training.with_column('Distance', all_distances(training, point))\n\ndef closest(training, point, k):\n    \"\"\"A table containing the k closest rows in the training table to array p.\"\"\"\n    with_dists = table_with_distances(training, point)\n    sorted_by_distance = with_dists.sort('Distance')\n    topk = sorted_by_distance.take(np.arange(k))\n    return topk\n\ndef majority(topkclasses):\n  \"\"\"1 if the majority of the \"Class\" column is 1s, and 0 otherwise.\"\"\"\n  ones = topkclasses.where('Class', are.equal_to(1)).num_rows\n  zeros = topkclasses.where('Class', are.equal_to(0)).num_rows\n  if ones &gt; zeros:\n    return 1\n  else:\n    return 0\n\ndef classify(training, p, k):\n  \"\"\"Classify an example with attributes p using k-nearest neighbor classification with the given training table.\"\"\"\n  closestk = closest(training, p, k)\n  topkclasses = closestk.select('Class')\n  return majority(topkclasses)\n\ndef classify_table(training, points, k):\n  \"\"\"Classify a table of unlabled points using KNN\"\"\"\n  def classify_p(p):\n    return classify(training, p, k)\n\n  classes = points.apply(classify_p)\n  return points.with_column('Class', classes)"
  },
  {
    "objectID": "projects/pokemon_legendary_classifier/index.html#classifying-legendary-pokemon",
    "href": "projects/pokemon_legendary_classifier/index.html#classifying-legendary-pokemon",
    "title": "Classifying Legendary Pokemon",
    "section": "Classifying Legendary Pokemon",
    "text": "Classifying Legendary Pokemon\n\nprediction_1 = classify_table(training, testing.drop(\"Class\"), 1)\nprediction_3 = classify_table(training, testing.drop(\"Class\"), 3)\nprediction_5 = classify_table(training, testing.drop(\"Class\"), 5)\n\n\naccuracy_1 = accuracy(prediction_1.column(\"Class\"), testing.column(\"Class\"))\naccuracy_3 = accuracy(prediction_3.column(\"Class\"), testing.column(\"Class\"))\naccuracy_5 = accuracy(prediction_5.column(\"Class\"), testing.column(\"Class\"))\n\nprint(\"k=1 -&gt;\", accuracy_1)\nprint(\"k=3 -&gt;\", accuracy_3)\nprint(\"k=5 -&gt;\", accuracy_5)\n\nk=1 -&gt; 0.9738805970149254\nk=3 -&gt; 0.9701492537313433\nk=5 -&gt; 0.9701492537313433\n\n\n\nResults\nThe classifier was run using a few different k-values for the k-nearest neighbor algorithm. Using the 1st nearest-neighbor, the accuracy was about 95%. Going to the 3 nearest neighbors, the accuracy went up to about 95.5%. With 5 nearest neighbors, the accuracy was 95.9%. Increasing the k-value just slightly increases the accuracy of the classifier."
  },
  {
    "objectID": "projects/spotify_classification_dashboard/index.html",
    "href": "projects/spotify_classification_dashboard/index.html",
    "title": "Spotify Classification Dashboard and Model Analysis",
    "section": "",
    "text": "include: \"/Users/brizaespinoza/Downloads/Git/suppiero.github.io/projects/spotify_classification_dashboard/spotify_song_classification.ipynb\"\n\n\n\n\nCitationBibTeX citation:@online{trujillo2024,\n  author = {Trujillo, Piero},\n  title = {Spotify {Classification} {Dashboard} and {Model} {Analysis}},\n  date = {2024-04-01},\n  url = {https://suppiero.github.io/projects/spotify_classification_dashboard/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTrujillo, Piero. 2024. ‚ÄúSpotify Classification Dashboard and Model\nAnalysis.‚Äù April 1, 2024. https://suppiero.github.io/projects/spotify_classification_dashboard/."
  },
  {
    "objectID": "projects/spotify_classification_dashboard/project.html",
    "href": "projects/spotify_classification_dashboard/project.html",
    "title": "Classification Dashboard and Model Analysis",
    "section": "",
    "text": "include: \"/Users/brizaespinoza/Downloads/Git/suppiero.github.io/projects/spotify_classification_dashboard/spotify_song_classification.ipynb\"\n\n\n\n\nCitationBibTeX citation:@online{trujillo2024,\n  author = {Trujillo, Piero},\n  title = {Classification {Dashboard} and {Model} {Analysis}},\n  date = {2024-04-01},\n  url = {https://suppiero.github.io/projects/spotify/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTrujillo, Piero. 2024. ‚ÄúClassification Dashboard and Model\nAnalysis.‚Äù April 1, 2024. https://suppiero.github.io/projects/spotify/."
  },
  {
    "objectID": "projects/spotify_classification_dashboard/spotify_song_classification.html",
    "href": "projects/spotify_classification_dashboard/spotify_song_classification.html",
    "title": "Introduction",
    "section": "",
    "text": "Spotify Classification Dashboard and Model Analysis\nIn this project, my friend Nirvit and I shared our 2023 Spotify Wrapped playlists so we could visualize comparisons between our music tastes and then create a model to try and predict whose playlist a song belongs to. Finally, I have compiled the results of each model into an interactive dashboard using Panel.\nThis blog post will have the following sections:\nNow, let‚Äôs dive into the exciting world of music data analysis!"
  },
  {
    "objectID": "projects/spotify_classification_dashboard/spotify_song_classification.html#setup-and-preprocessing",
    "href": "projects/spotify_classification_dashboard/spotify_song_classification.html#setup-and-preprocessing",
    "title": "Introduction",
    "section": "Setup and Preprocessing",
    "text": "Setup and Preprocessing\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Read in csv file to create tabular dataframe \npiero_top_songs = pd.read_csv(\"/Users/piero/Downloads/Spotify_Project/Piero_Top_Songs_2023.csv\") \nnirvit_top_songs = pd.read_csv(\"/Users/piero/Downloads/Spotify_Project/Nirvit_Top_Songs_2023.csv\") \n\n# Add a truth column to classify whether a song is from Piero's or Nirvit's playlist\npiero_top_songs['Playlist Owner'] = 'Piero'\nnirvit_top_songs['Playlist Owner'] = 'Nirvit'\n\n# Convert time to seconds\npiero_top_songs['Time Seconds'] = pd.to_timedelta('00:' + piero_top_songs['Time']).dt.total_seconds().astype(int)\nnirvit_top_songs['Time Seconds'] = pd.to_timedelta('00:' + nirvit_top_songs['Time']).dt.total_seconds().astype(int)\n\n# Remove unnecessary columns\npiero_top_songs = piero_top_songs.drop(columns=['Song Preview', 'Spotify Track Img', 'Album Label', 'Spotify Track Id', 'Added At', 'Spotify Track Id', '#', 'Album', 'Album Date', 'Time'])\nnirvit_top_songs = nirvit_top_songs.drop(columns=['Song Preview', 'Spotify Track Img', 'Album Label', 'Spotify Track Id', 'Added At', 'Spotify Track Id', '#', 'Album', 'Album Date', 'Time'])\n\n# Join playlists into one dataframe\nall_songs = pd.concat([piero_top_songs, nirvit_top_songs])\n\n# Convert all object columns to type string\nobject_columns = all_songs.select_dtypes(include=['object']).columns # First, create list of object columns to convert\nall_songs[object_columns] = all_songs[object_columns].astype('string')\n\n#print(all_songs.dtypes) # Check that column types have been converted to string\n\n\n# Check for null values\nall_songs.isnull().sum().sum() # 9 NaN values in 'Genres' and 'Parent Genres' columns\n\n# Create dataframe of songs containing NaN values in either 'Genres' or 'Parent Genres'\nnan_rows = all_songs[(all_songs['Genres'].isnull()) | (all_songs['Parent Genres'].isnull())]\n\n# Fill NaNs in 'Parent Genres' column with 'Unknown' since I cannot find them on Spotify or Google\nall_songs[['Genres']] = all_songs[['Genres']].fillna('Unknown')\n\n# Populate NaN values in 'Genres' column with genres found on Spotify or Google for specified song and artist\nall_songs.loc[(all_songs['Song'] == 'Mumbo Sugar') & (all_songs['Artist'] == 'Arc De Soleil'), ['Parent Genres']] = ['R&B, Soul']\n\nall_songs.loc[(all_songs['Song'] == 'Give It Back') & (all_songs['Artist'] == 'Gaelle'), ['Parent Genres']] = ['Dance, Electronic']\n\nall_songs.loc[(all_songs['Song'] == 'ÊÑõ„Åó„Å¶„Çã') & (all_songs['Artist'] == \"callin'\"), ['Parent Genres']] = ['Anime, J-Pop']\n\nall_songs.loc[(all_songs['Song'] == 'You Are Mine') & (all_songs['Artist'] == 'Jay Robinson'), ['Parent Genres']] = ['Classic Soul']\n\nall_songs.loc[(all_songs['Song'] == 'Thank You DubNation! (the page will never be long enough)') & (all_songs['Artist'] == 'herlovebeheadsdaisies'), ['Parent Genres']] = ['Screamo']\n\n# Convert categorical variables to factors - allow us to use non-numeric data in statistical modeling\nobject_columns = all_songs.select_dtypes(include=['object']).columns # First, create list of object columns to convert\nall_songs[object_columns] = all_songs[object_columns].astype('category')\n\n\n# Making sure there are no null values left in the dataset\nnan_rows = all_songs[(all_songs['Genres'].isnull()) | (all_songs['Parent Genres'].isnull())]\nnan_rows\n\n\n\n\n\n\n\n\nSong\nArtist\nPopularity\nBPM\nGenres\nParent Genres\nDance\nEnergy\nAcoustic\nInstrumental\nHappy\nSpeech\nLive\nLoud\nKey\nTime Signature\nCamelot\nPlaylist Owner\nTime Seconds\n\n\n\n\n\n\n\n\n\n\n# Splitting 'Parent Genres' column since there are so many different genres\nfirst_instance = all_songs['Parent Genres'].str.split(',').str[0] # extract first genre element\n\n# Assign first instance to new 'Genre' column\nall_songs['Genre'] = first_instance\n\nunique_genres = all_songs['Genre'].unique()\nnum_unique_genres = len(unique_genres)\nprint(\"Number of unique genres:\", num_unique_genres)\n\n# Counting unique genres\nall_songs['Genre'].value_counts() # 17 (now) vs 56 (before)\n\n# Remove unnecessary columns\nall_songs = all_songs.drop(columns=['Parent Genres', 'Genres'])\n\nNumber of unique genres: 17\n\n\n\nprint(all_songs.dtypes) # Check categorical column types have been converted to string\n\nSong              string\nArtist            string\nPopularity         int64\nBPM                int64\nDance              int64\nEnergy             int64\nAcoustic           int64\nInstrumental       int64\nHappy              int64\nSpeech             int64\nLive               int64\nLoud               int64\nKey               string\nTime Signature     int64\nCamelot           string\nPlaylist Owner    string\nTime Seconds       int64\nGenre             object\ndtype: object\n\n\n\nFinal Dataset\n\n# Save dataset as csv file\n#all_songs.to_csv('all_spotify_songs.csv')\n\n# Final dataset\nall_songs\n\n\n\n\n\n\n\n\nSong\nArtist\nPopularity\nBPM\nDance\nEnergy\nAcoustic\nInstrumental\nHappy\nSpeech\nLive\nLoud\nKey\nTime Signature\nCamelot\nPlaylist Owner\nTime Seconds\nGenre\n\n\n\n\n0\nCAN'T SAY\nTravis Scott\n80\n148\n70\n71\n20\n0\n71\n0\n10\n-5\nA#/B‚ô≠ Minor\n4\n3A\nPiero\n198\nHip Hop\n\n\n1\nNew Gold (feat. Tame Impala and Bootie Brown)\nGorillaz,Tame Impala,Bootie Brown\n71\n108\n70\n92\n4\n5\n55\n0\n10\n-4\nC‚ôØ/D‚ô≠ Minor\n3\n12A\nPiero\n215\nHip Hop\n\n\n2\n1AM FREESTYLE\nJoji\n68\n126\n62\n54\n75\n0\n12\n0\n10\n-6\nC Minor\n4\n5A\nPiero\n113\nPop\n\n\n3\n20 Min\nLil Uzi Vert\n84\n123\n77\n75\n11\n0\n78\n10\n10\n-4\nG#/A‚ô≠ Minor\n4\n1A\nPiero\n220\nHip Hop\n\n\n4\nThe Less I Know The Better\nTame Impala\n88\n117\n64\n74\n1\n1\n79\n0\n10\n-4\nE Major\n4\n12B\nPiero\n216\nMetal\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\nFLASH CASANOVA\nYabujin\n53\n143\n42\n72\n1\n0\n41\n10\n0\n-10\nC‚ôØ/D‚ô≠ Major\n4\n3B\nNirvit\n163\nHip Hop\n\n\n96\nSinceramente\nS√©rgio Sampaio\n51\n92\n71\n25\n94\n0\n85\n0\n10\n-11\nE Minor\n4\n9A\nNirvit\n78\nJazz\n\n\n97\n24 Hr Drive-Thru\nOrigami Angel\n52\n155\n57\n96\n2\n0\n26\n10\n30\n-4\nG#/A‚ô≠ Major\n4\n4B\nNirvit\n164\nRock\n\n\n98\nIf I Ain't Got You\nAlicia Keys\n84\n118\n61\n44\n60\n0\n17\n10\n10\n-9\nG Major\n3\n9B\nNirvit\n228\nR&B\n\n\n99\nSolitude\nLord Snow\n20\n85\n16\n99\n0\n9\n9\n30\n40\n-5\nA Minor\n4\n8A\nNirvit\n312\nMetal\n\n\n\n\n200 rows √ó 18 columns"
  },
  {
    "objectID": "projects/spotify_classification_dashboard/spotify_song_classification.html#exploratory-data-analysis-for-feature-selection",
    "href": "projects/spotify_classification_dashboard/spotify_song_classification.html#exploratory-data-analysis-for-feature-selection",
    "title": "Introduction",
    "section": "Exploratory Data Analysis for Feature Selection",
    "text": "Exploratory Data Analysis for Feature Selection\n\nCorrelation Heatmap\n\nimport plotly.graph_objects as go\n\ndef corr_plot(data):\n    # Calculate the correlation matrix\n    correlation_matrix = data.corr()\n\n    # Create heatmap using Plotly  \n    annotations = []\n    for i, row in enumerate(correlation_matrix.values):\n        for j, value in enumerate(row):\n            font_color = 'white' if value &gt; -0.4 else '#7fc591'  # Set font color based on z value\n            annotations.append(dict(x=correlation_matrix.columns[j], y=correlation_matrix.index[i],\n                                text=str(round(value, 2)),\n                                showarrow=False, font=dict(color=font_color)))\n\n    # Create heatmap using Plotly\n    fig = go.Figure(data=go.Heatmap(\n                    z=correlation_matrix.values,\n                    x=correlation_matrix.columns,\n                    y=correlation_matrix.index,\n                    colorscale='Greens',  # Choose your preferred colorscale\n                    colorbar=dict(title='Correlation&lt;br&gt;Strength&lt;br&gt;')\n    ))\n\n\n\n    fig.update_layout(\n        title=dict(text ='&lt;b&gt;Correlation Heatmap&lt;/b&gt;', x=0.5, y=0.85),\n        xaxis=dict(title='&lt;b&gt;Features&lt;/b&gt;'),\n        yaxis=dict(title='&lt;b&gt;Features&lt;/b&gt;'),\n        annotations=annotations,\n        template=\"plotly_dark\",\n        height=500,\n        width=700,\n        hoverlabel=dict(\n            bgcolor=\"#008000\")\n    )\n\n    return fig\n\ncorr_plot(all_songs)\n\n/var/folders/th/h7c9tz61505fhg7ds00qjhlm0000gn/T/ipykernel_1168/3196471437.py:5: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = data.corr()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nA correlation heatmap visualizes how well different variables interact with each other. By illustrating the strength and direction of these relationships, correlation heatmaps help identify patterns, trends, and dependencies within the data. Therefore, we are most interested in the features with very dark or very light tiles.\nA few main takeaways: * Loud: Strong positive correlation with Energy suggests that louder songs tend to have higher energy levels.\n\nAccoustic: Strong negative correlation with Energy and Loudness implies that acoustic songs tend to have lower energy and loudness levels.\nEnergy: Strong positive correlation with Loudness indicates that energetically intense songs tend to be louder.\nInstrumental: Moderate negative correlation with Danceability and Popularity suggests that instrumental songs are less danceable and less popular.\nDance: It has a moderate positive correlation with Popularity, Energy, and Happiness, suggesting that more danceable songs tend to be more popular, energetic, and happier.\nPopularity: It shows weak positive correlations with attributes like Dance, Energy, Happy, and Loudness, indicating that more popular songs tend to have higher danceability, energy, happiness, and loudness.\n\n\n\nInteractive Scatterplot Comparing Similarity Between Music Tastes\n\nimport plotly.graph_objects as go\n\n# Define colors for each playlist owner\ncolor_map = {'Piero': '#1ED760', 'Nirvit': '#ff00ff'} #1db96e , #b91d82\n\n# Define symbols for each playlist owner\nsymbol_map = {'Piero': 'circle', 'Nirvit': 'diamond'} #triangle-up\n\n# Define a function to create scatter plot with my original dataset\ndef create_original_scatter_plot(all_songs):\n    # Create scatter plot\n    fig = go.Figure()\n\n    # Add text markers when hovering over points\n    for group, data in all_songs.groupby('Playlist Owner'):\n        fig.add_trace(go.Scatter(\n            x=data['Happy'],\n            y=data['Energy'],\n            opacity=0.75,\n            mode='markers',\n            name=group,\n            text=data.apply(lambda row: f\"Song: {row['Song']}, Artist: {row['Artist']}, Energy: {row['Energy']}, Happiness: {row['Happy']}\", axis=1),  # Hover text\n            marker=dict(\n                color=color_map[group],  # Color points based on group\n                size=10,\n                symbol=symbol_map.get(group, 'circle'),\n                line=dict(\n                    color='#2a8ccb',\n                    width=2\n                )\n            )\n        ))\n\n    # Scatterplot layout\n    fig.update_layout(\n        title={\n            'text': \"&lt;b&gt;Top 100 Songs by Mood&lt;/b&gt;\", # Top 100 Songs by Positivity and Energy Levels\n            'font': {'size': 14},\n            'x': 0.5,  # Centered title\n            'y': 0.9  # Adjust vertical position of title\n        },\n        xaxis_title=\"Happiness Level\",\n        yaxis_title=\"Energy Level\",\n        legend_title=\"Listener\",\n        width=1070,  # Set width to 1000 pixels\n        height=525,  # Set height to 600 pixels\n        template=\"plotly_dark\",\n        # Make hover text white\n        hoverlabel=dict( \n            font=dict(\n                color=\"white\"  # Text color inside hover label\n            ))\n        \n    )\n\n\n    # Label song mood quadrants\n    fig.add_annotation(\n        x=0, y=105,\n        text=\"&lt;b&gt;Chaotic/Angry&lt;/b&gt;\",\n        font=dict(\n            size=12,\n            color=\"white\",\n        ),\n        showarrow=False\n    ) \n\n    fig.add_annotation(\n        x= 100, y=105,\n        text=\"&lt;b&gt;Happy/Upbeat&lt;/b&gt;\",\n        font=dict(\n            size=12,\n            color=\"white\"\n        ),\n        showarrow=False\n    )\n\n    fig.add_annotation(\n        x= 100, y=-5,\n        text=\"&lt;b&gt;Chill/Peaceful&lt;/b&gt;\",\n        font=dict(\n            size=12,\n            color=\"white\"\n        ),\n        showarrow=False\n    )\n\n    fig.add_annotation(\n        x=0, y=-5,\n        text=\"&lt;b&gt;Sad/Depressing&lt;/b&gt;\",\n        font=dict(\n            size=12,\n            color=\"white\"\n        ),\n        showarrow=False\n    )\n\n    # Adding cross section to distinguish mood sectors\n\n    # Vertical line\n    fig.add_shape(\n        type=\"line\",\n        x0=50, y0=0,\n        x1=50, y1=100,\n        line=dict(\n            color=\"white\",\n            width=1,\n            dash=\"dash\"\n        )\n    )\n\n    # Horizontal line\n    fig.add_shape(\n        type=\"line\",\n        x0=0, y0=50,\n        x1=100, y1=50,\n        line=dict(\n            color=\"white\",\n            width=1,\n            dash=\"dash\"\n        )\n    )\n\n    # Show the plot\n    return fig\n\ncreate_original_scatter_plot(all_songs)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nThis scatterplot compares the energy and happiness levels of all songs in our Spotify Wrapped playlists. To interpret the plot, it‚Äôs important to think about how energy and happiness features interact.\n\nLow Energy + Low Happiness = Sad / Depressing\nLow Energy + High Happiness = Chill / Peaceful\nHigh Energy + High Happiness = Happy / Upbeat\nHigh Energy + Low Happiness = Chaotic / Angry\n\nThe scatterplot reveals that songs from my playlist are primarily clustered in the top quadrant, reflecting a mix of chaotic/angry and happy/upbeat tunes. This clustering pattern could significantly influence a model‚Äôs predictive capabilities, potentially making the dataset more predictable than anticipated. Additionally, another notable trend emerges: while Nirvit‚Äôs music taste appears evenly spread across the plot, he tends to gravitate towards a higher proportion of sad and chill music compared to my preferences.\nMake sure to hover over the various points on the scatterplot, to see which songs they represent."
  },
  {
    "objectID": "projects/spotify_classification_dashboard/spotify_song_classification.html#prepping-data-for-machine-learning-models",
    "href": "projects/spotify_classification_dashboard/spotify_song_classification.html#prepping-data-for-machine-learning-models",
    "title": "Introduction",
    "section": "Prepping Data For Machine Learning Models",
    "text": "Prepping Data For Machine Learning Models\n\nNormalize Data\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Remove Artist and Song columns\nnormalized_songs = all_songs.drop(columns=['Song', 'Artist'])\n\n# Select numerical columns to normalize\ncolumns_to_normalize = ['Popularity', 'BPM', 'Dance', 'Energy', 'Acoustic', 'Instrumental', 'Happy', 'Speech', 'Live', 'Loud', 'Time Signature', 'Time Seconds']\n\n# Initialize MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit the scaler on the selected columns\nscaler.fit(normalized_songs[columns_to_normalize])\n\n# Transform the selected columns\nnormalized_songs[columns_to_normalize] = scaler.transform(normalized_songs[columns_to_normalize])\n\n# Create a new binary response column\nnormalized_songs['Binary Response'] = (normalized_songs['Playlist Owner'] == 'Piero').astype(int)\n\n# Drop the original 'playlist' column if no longer needed\nnormalized_songs.drop(columns=['Playlist Owner'], inplace=True)\n\nNow we‚Äôve got ourselves a normalized dataset!\n\nnormalized_songs\n\n\n\n\n\n\n\n\nPopularity\nBPM\nDance\nEnergy\nAcoustic\nInstrumental\nHappy\nSpeech\nLive\nLoud\nKey\nTime Signature\nCamelot\nTime Seconds\nGenre\nBinary Response\n\n\n\n\n0\n0.857143\n0.550725\n0.732558\n0.707071\n0.202020\n0.000000\n0.715789\n0.000000\n0.125\n0.820513\nA#/B‚ô≠ Minor\n0.75\n3A\n0.115156\nHip Hop\n1\n\n\n1\n0.750000\n0.260870\n0.732558\n0.919192\n0.040404\n0.050505\n0.547368\n0.000000\n0.125\n0.846154\nC‚ôØ/D‚ô≠ Minor\n0.50\n12A\n0.127786\nHip Hop\n1\n\n\n2\n0.714286\n0.391304\n0.639535\n0.535354\n0.757576\n0.000000\n0.094737\n0.000000\n0.125\n0.794872\nC Minor\n0.75\n5A\n0.052006\nPop\n1\n\n\n3\n0.904762\n0.369565\n0.813953\n0.747475\n0.111111\n0.000000\n0.789474\n0.166667\n0.125\n0.846154\nG#/A‚ô≠ Minor\n0.75\n1A\n0.131501\nHip Hop\n1\n\n\n4\n0.952381\n0.326087\n0.662791\n0.737374\n0.010101\n0.010101\n0.800000\n0.000000\n0.125\n0.846154\nE Major\n0.75\n12B\n0.128529\nMetal\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n0.535714\n0.514493\n0.406977\n0.717172\n0.010101\n0.000000\n0.400000\n0.166667\n0.000\n0.692308\nC‚ôØ/D‚ô≠ Major\n0.75\n3B\n0.089153\nHip Hop\n0\n\n\n96\n0.511905\n0.144928\n0.744186\n0.242424\n0.949495\n0.000000\n0.863158\n0.000000\n0.125\n0.666667\nE Minor\n0.75\n9A\n0.026003\nJazz\n0\n\n\n97\n0.523810\n0.601449\n0.581395\n0.959596\n0.020202\n0.000000\n0.242105\n0.166667\n0.375\n0.846154\nG#/A‚ô≠ Major\n0.75\n4B\n0.089896\nRock\n0\n\n\n98\n0.904762\n0.333333\n0.627907\n0.434343\n0.606061\n0.000000\n0.147368\n0.166667\n0.125\n0.717949\nG Major\n0.50\n9B\n0.137444\nR&B\n0\n\n\n99\n0.142857\n0.094203\n0.104651\n0.989899\n0.000000\n0.090909\n0.063158\n0.500000\n0.500\n0.820513\nA Minor\n0.75\n8A\n0.199851\nMetal\n0\n\n\n\n\n200 rows √ó 16 columns\n\n\n\n\n\nSetting Up Training and Testing Data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Define features (X) and target variable (y)\nX = normalized_songs[['Popularity', 'BPM', 'Dance', 'Energy', 'Acoustic', 'Instrumental', 'Happy', 'Speech', 'Live', 'Loud', 'Key', 'Time Signature', 'Camelot', 'Time Seconds', 'Genre']] # Features\n\ny = normalized_songs['Binary Response'] # Target variable Playlist Owner\n\n# Initialize OneHotEncoder\nencoder = OneHotEncoder(drop='first', sparse=False)\n\n# One-hot encode categorical columns\nX_encoded = pd.DataFrame(encoder.fit_transform(X[['Key', 'Camelot', 'Genre']]))  # Only encode categorical columns\nX_encoded.columns = encoder.get_feature_names_out(['Key', 'Camelot', 'Genre'])  # Get categorical column names\n\n# Reset indices of X and X_encoded\nX.reset_index(drop=True, inplace=True)\nX_encoded.reset_index(drop=True, inplace=True)\n\n# Concatenate numerical and encoded categorical columns\nX_final = pd.concat([X, X_encoded], axis=1)\n\n# Drop original columns since they have been encoded to new columns\nX_final.drop(columns=['Key', 'Camelot', 'Genre'], inplace=True)\n\n# Splitting up the data into training and testing sets (60% training, 40% testing)\nX_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.4, random_state=18, shuffle=True)\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n\n\n\nNow that the data has been split into testing and training sets, the next step involves creating machine learning models to predict which Spotify Wrapped playlist a song belongs to."
  },
  {
    "objectID": "projects/spotify_classification_dashboard/spotify_song_classification.html#creating-machine-learning-models",
    "href": "projects/spotify_classification_dashboard/spotify_song_classification.html#creating-machine-learning-models",
    "title": "Introduction",
    "section": "Creating Machine Learning Models",
    "text": "Creating Machine Learning Models\n\nLogistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Create and train the logistic regression model\nlr_model = LogisticRegression()\nlr_model.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred_lr = lr_model.predict(X_test)\n\n# Evaluate the model\naccuracy_lr = accuracy_score(y_test, y_pred_lr)\nprint(\"Accuracy:\", accuracy_lr)\n\nAccuracy: 0.7375\n\n\n\n\nFeature Importance Plot (Logistic Regression)\nI developed this feature importance plot function to identify the most and least useful predictors in each model.\n\nimport plotly.graph_objects as go\nimport panel as pn\n\ndef plot_linear_feature_importance(model_name):\n    # Get feature importances\n    lr_importances = model_name.coef_[0]\n    indices = np.argsort(lr_importances)[::-1]\n\n    # Get feature names\n    feature_names = X_train.columns\n\n    # Create custom color gradient\n    colors = ['#1DB954', '#2BBE60', '#3AC26C', '#48C778', '#57CB84', '#65D08F', '#74D49B', '#83D9A7', '#91DDB3', '#9FE2BF'] \n\n    # Create figure\n    fig = go.Figure()\n\n    # Add bars to plot\n    fig.add_trace(go.Bar(\n        x=lr_importances[indices][:10],  # Grabs the top 10 features\n        y=[feature_names[i] for i in indices[:10]],  # Grabs their corresponding feature names\n        marker=dict(color=colors),\n        orientation='h'  # Style as horizontal bar chart\n    ))\n\n    # Style barplot\n    fig.update_layout(\n        title=dict(text=\"&lt;b&gt;Top 10 Feature Importances&lt;/b&gt;\", x=0.5, font=dict(size=16, color='white', family='Arial, sans-serif')),\n        xaxis=dict(title='&lt;b&gt;Importance&lt;/b&gt;', titlefont=dict(size=14, color='white', family='Arial, sans-serif')),\n        yaxis=dict(title='&lt;b&gt;Features&lt;/b&gt;', titlefont=dict(size=14, color='white', family='Arial, sans-serif')),\n        font=dict(size=12, color='white', family='Arial, sans-serif'),\n        margin=dict(l=100, r=20, t=40, b=20),\n        height=500, #500\n        width=700,  # 800\n        template=\"plotly_dark\", # dark mode\n         # Make hover markers have white text\n        hoverlabel=dict(\n            font=dict(\n                color=\"white\"\n            )\n        )\n    )\n\n    return fig # Display plot in dashboard when clicked\n\n# Call function for logistic regression\nplot_linear_feature_importance(lr_model)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCreating a Visualization Dataset\nTo craft scatterplots, we need a streamlined visualization dataset containing only essential columns. This dataset, labeled viz_dataset, is extracted from the original dataset, all_songs, and encompasses descriptive song attributes like ‚ÄòSong‚Äô, ‚ÄòArtist‚Äô, ‚ÄòPlaylist Owner‚Äô, in addition to ‚ÄòHappy‚Äô and ‚ÄòEnergy‚Äô levels. The extraction process involves selecting rows corresponding to indices found within the X_test dataset.\n\n# Reset index of the all_songs DataFrame\nall_songs_reset_index = all_songs.reset_index(drop=True)\n\n# Extract rows from the original dataset based on indices in X_test\nviz_dataset = all_songs_reset_index.loc[X_test.index, ['Song', 'Artist', 'Playlist Owner','Happy', 'Energy']]\n\nviz_dataset\n\n\n\n\n\n\n\n\nSong\nArtist\nPlaylist Owner\nHappy\nEnergy\n\n\n\n\n134\nSuite bergamasque, L. 75: III. Clair de lune\nClaude Debussy,Philippe Entremont\nNirvit\n4\n6\n\n\n91\nFair Trade (with Travis Scott)\nDrake,Travis Scott\nPiero\n29\n47\n\n\n81\nFather Stretch My Hands Pt. 1\nKanye West\nPiero\n44\n57\n\n\n108\nÊÑõ„Åó„Å¶„Çã\ncallin'\nNirvit\n31\n31\n\n\n170\nDisfar√ßa E Chora\nCartola\nNirvit\n96\n44\n\n\n...\n...\n...\n...\n...\n...\n\n\n126\nKiss the Ladder\nFleshwater\nNirvit\n25\n99\n\n\n37\nlose\nTravis Scott\nPiero\n28\n56\n\n\n27\nDoin' it Right (feat. Panda Bear)\nDaft Punk,Panda Bear\nPiero\n19\n45\n\n\n2\n1AM FREESTYLE\nJoji\nPiero\n12\n54\n\n\n77\nHot Air Balloon\nDon Diablo,AR/CO\nPiero\n57\n71\n\n\n\n\n80 rows √ó 5 columns\n\n\n\n\n\nCreating a Scatterplot Function to Show Logistic Regression Classification Results\nThis function can create scatterplots for any type of model, whether it‚Äôs linear, tree-based, or cluster-based. The plan is to utilize it in the dashboard to visually represent classification song predictions for every model.\n\nimport plotly.graph_objects as go\n\ndef model_plot(y_pred):\n\n    # Define colors for each playlist owner\n    color_map = {1: '#1ED760', 0: '#ff00ff'} #1db96e , #b91d82\n\n    # Define symbols for each playlist owner\n    symbol_map = {1: 'circle', 0: 'diamond'} \n\n    # Map class labels to name legend labels\n    legend_labels = {1: 'Piero', 0: 'Nirvit'}\n\n    # Replace prediction labels (1,0) for names (Piero, Nirvit) in the legend\n    legend_names = [legend_labels[label] for label in color_map.keys()]\n\n    # Add truth labels by merging `y_pred` from each model as a prediction column\n    viz_dataset['Predicted Owner'] = y_pred\n\n    # Create scatter plot\n    fig = go.Figure()\n\n    # Add text markers when hovering over points\n    for group, data in viz_dataset.groupby('Predicted Owner'):\n        fig.add_trace(go.Scatter(\n            x=data['Happy'],\n            y=data['Energy'],\n            opacity=0.75,\n            mode='markers',\n            name=legend_labels[group],\n            text=data.apply(lambda row: f\"Song: {row['Song']}, Artist: {row['Artist']}, Energy: {row['Energy']}, Happiness: {row['Happy']}\", axis=1),  # Hover text\n            marker=dict(\n                color=color_map[group],  # Color points based on group\n                size=10,\n                symbol=symbol_map.get(group, 'circle'),\n                line=dict(\n                    color='#2a8ccb', ##2a8ccb\n                    width=2\n                )\n            )\n        ))\n\n\n    # Change scatterplot appearance / styles\n    fig.update_layout(\n         title={\n        'text': \"&lt;b&gt;Top 100 Songs by Mood&lt;/b&gt;\", # Top 100 Songs by Positivity and Energy Levels\n        'font': {'size': 14},\n        'x': 0.5,  # Centered title\n        'y': 0.9  # Adjust vertical position of title\n        },\n        xaxis_title=\"Happiness Level\",\n        yaxis_title=\"Energy Level\",\n        legend_title=\"Listener\",\n        width=1070,\n        height=525,\n        template=\"plotly_dark\",\n        # Make hover text white\n        hoverlabel=dict(\n            font=dict(\n                color=\"white\"\n            )\n        )\n       \n    )\n\n    # Label song mood quadrants\n    fig.add_annotation(\n        x=0, y=105,\n        text=\"&lt;b&gt;Chaotic/Angry&lt;/b&gt;\",\n        font=dict(\n            size=12,\n            color=\"white\"\n        ),\n        showarrow=False\n    )\n\n    fig.add_annotation(\n        x= 100, y=105,\n        text=\"&lt;b&gt;Happy/Upbeat&lt;/b&gt;\",\n        font=dict(\n            size=12,\n            color=\"white\"\n        ),\n        showarrow=False\n    )\n\n\n    fig.add_annotation(\n        x= 100, y=-5,\n        text=\"&lt;b&gt;Chill/Peaceful&lt;/b&gt;\",\n        font=dict(\n            size=12,\n            color=\"white\"\n        ),\n        showarrow=False\n    )\n\n    fig.add_annotation(\n        x=0, y=-5,\n        text=\"&lt;b&gt;Sad/Depressing&lt;/b&gt;\",\n        font=dict(\n            size=12,\n            color=\"white\"\n        ),\n        showarrow=False\n    )\n\n    # Adding cross section to distinguish mood sectors\n\n    # Vertical line\n    fig.add_shape(\n        type=\"line\",\n        x0=50, y0=0,\n        x1=50, y1=100,\n        line=dict(\n            color=\"white\",\n            width=1,\n            dash=\"dash\"\n        )\n    )\n\n    # Horizontal line\n    fig.add_shape(\n        type=\"line\",\n        x0=0, y0=50,\n        x1=100, y1=50,\n        line=dict(\n            color=\"white\",\n            width=1,\n            dash=\"dash\"\n        )\n    )\n\n    # Show scatterplot\n    return fig \n\n\nmodel_plot(y_pred_lr)  # Scatterplot for logistic regression\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nRandom Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\n# Create random forest model\nrf_model = RandomForestClassifier(n_estimators=1000, random_state=18)\n\n# Train Model\nrf_model.fit(X_train, y_train)\n\n# Predictions\ny_pred_rf = rf_model.predict(X_test)\n\n# Evaluate model performance\nrf_accuracy = accuracy_score(y_test, y_pred_rf)\nprint(\"Accuracy:\", rf_accuracy)\n\nAccuracy: 0.7875\n\n\n\n\nFeature Importance Plot (Random Forest)\nSince linear models and tree-based models store their feature importances differently, two separate feature importance plot functions are required.\nIn linear models, such as linear regression or logistic regression, feature importance is derived directly from the coefficients assigned to each feature during the model fitting process. These coefficients represent the magnitude and direction of the relationship between each feature and the target variable. Therefore, accessing the .coef_ attribute retrieves these coefficients, which can be interpreted as feature importances.\nIn tree-based models like Random Forests, feature importance is typically computed based on how much each feature contributes to decreasing impurity (e.g., Gini impurity or entropy) across all the trees in the forest. The .feature_importances_ attribute of a trained Random Forest model provides the importance scores for each feature, calculated based on this criterion.\nSo, while linear models directly use the coefficients as feature importance, Random Forest models use a measure of impurity decrease to determine feature importance across the ensemble of trees.\n\nimport plotly.graph_objects as go\nimport panel as pn\n\ndef plot_tree_feature_importance(model_name):\n    # Get feature importances for tree-based model\n    lr_importances = model_name.feature_importances_\n    indices = np.argsort(lr_importances)[::-1]\n\n    # Get corresponding feature names\n    feature_names = X_train.columns\n\n    # Create custom color gradient\n    colors = ['#1DB954', '#2BBE60', '#3AC26C', '#48C778', '#57CB84', '#65D08F', '#74D49B', '#83D9A7', '#91DDB3', '#9FE2BF'] \n\n    # Create figure\n    fig = go.Figure()\n\n    # Add bars to plot\n    fig.add_trace(go.Bar(\n        x=lr_importances[indices][:10],  # Grab top 10 features in the model\n        y=[feature_names[i] for i in indices[:10]],  # Get corresponding feature names\n        marker=dict(color=colors), # assign color gradient to bars\n        orientation='h'  # Style as horizontal barplot\n    ))\n\n    # Style barplot\n    fig.update_layout(\n        title=dict(text=\"&lt;b&gt;Top 10 Feature Importances&lt;/b&gt;\", x=0.5, font=dict(size=16, color='white', family='Arial, sans-serif')),\n        xaxis=dict(title='&lt;b&gt;Importance&lt;/b&gt;', titlefont=dict(size=14, color='white', family='Arial, sans-serif')),\n        yaxis=dict(title='&lt;b&gt;Features&lt;/b&gt;', titlefont=dict(size=14, color='white', family='Arial, sans-serif')),\n        font=dict(size=12, color='white', family='Arial, sans-serif'),\n        margin=dict(l=100, r=20, t=40, b=20),\n        height=500,\n        width=700,\n        template=\"plotly_dark\",\n         # Make hover text white\n        hoverlabel=dict(\n            font=dict(\n                color=\"white\"\n            )\n        )\n    )\n\n    return fig # display plot in dashboard when clicked\n\n# Plot random forest barplot\nplot_tree_feature_importance(rf_model)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nBoosted Trees\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Create boosted trees model\nboost_model = GradientBoostingClassifier(n_estimators=1000,\n                                max_depth=3,\n                                learning_rate=0.1,\n                                min_samples_split=3)\n\n# Fit the model to training set\nboost_model.fit(X_train, y_train)\n\n# Predictions\ny_pred_boost = boost_model.predict(X_test)\n\n# Evaluate boosted trees model accuracy\nboost_accuracy = accuracy_score(y_test, y_pred_boost)\nprint(\"Accuracy:\", boost_accuracy)\n\nAccuracy: 0.8\n\n\n\n\nFeature Importance Plot (Boosted Trees)\n\nplot_tree_feature_importance(boost_model)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nK-Nearest Neighbors (KNN)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Create K-nearest neighbors classifier\nknn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as needed\n\n# Fit the model to training set\nknn_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_knn = knn_model.predict(X_test)\n\n# Calculate accuracy\nknn_accuracy = accuracy_score(y_test, y_pred_knn)\nprint(\"Accuracy:\", knn_accuracy)\n\nAccuracy: 0.6375\n\n\n\n\nFeature Importance Plot (KNN)\nUnfortunately, a feature importance bar plot cannot be plotted because the K-Nearest Neighbors algorithm doesn‚Äôt inherently provide feature importance scores like tree-based algorithms or linear models. Instead, K-Nearest Neighbors is a distance-based algorithm that makes predictions using Euclidean distance to measure proximity and similarity between data points. Due to the lack of feature importance scores and its low performance, it will not be included in the final dashboard.\n\n\nSupport Vector Machine\n\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Create Support Vector Machine Classifier\nsvm_model = SVC(kernel='linear')  # Other kernels I could choose 'linear', 'rbf', 'poly'\n\n# Fit the model to training set\nsvm_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_svm = svm_model.predict(X_test)\n\n# Calculate accuracy\nsvm_accuracy = accuracy_score(y_test, y_pred_svm)\nprint(\"Accuracy:\", svm_accuracy)\n\nAccuracy: 0.75\n\n\n\n\nFeature Importance Plot (SVM)\n\nplot_linear_feature_importance(svm_model)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nDecision Trees\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Create Decision Tree Classifier\ndec_tree_model = DecisionTreeClassifier()\n\n# Fit the model to training set\ndec_tree_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_dec_tree = dec_tree_model.predict(X_test)\n\n# Calculate accuracy\ndec_tree_accuracy = accuracy_score(y_test, y_pred_dec_tree)\nprint(\"Accuracy:\", dec_tree_accuracy)\n\nAccuracy: 0.7875\n\n\n\n\nFeature Importance Plot (Decision Tree)\n\nplot_tree_feature_importance(dec_tree_model)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nGauge Visualization\nNow, a gauge visualization function is developed to showcase model accuracy on the dashboard.\n\nimport panel as pn\nimport plotly.graph_objects as go\n\n# Create gauge visualization function\ndef gauge_accuracy_viz(model_performance, last_reference):\n    # Calculate delta to show if current model is performing better or worse\n    delta = model_performance - last_reference\n\n    # Create gauge chart\n    fig = go.Figure(go.Indicator(\n        mode=\"gauge+number+delta\",\n        value= model_performance * 100,\n        domain={'x': [0, 1], 'y': [0, 1]},\n        title={'text': \"Accuracy\", 'font': {'size': 24, 'color': \"#00ff7f\"}},\n        delta={'reference': last_reference * 100, 'increasing': {'color': \"#00ff00\"}, 'decreasing': {'color': \"#ff7373\"}},\n        gauge={\n            'axis': {'range': [None, 100], 'tickwidth': 2, 'tickcolor': \"#70D2A2\"},\n            'bar': {'color': \"#1DB954\"},\n            'bgcolor': \"white\",\n            'borderwidth': 3,\n            'bordercolor': \"#00ff7f\",\n            'steps': [\n                {'range': [0, 50], 'color': '#b91d82'},\n                {'range': [50, 100], 'color': '#fff68f'}],\n            'threshold': {\n                'line': {'color': \"#cc0000\", 'width': 4},\n                'thickness': 0.75,\n                'value': model_performance * 100}}\n    ))\n    \n    # Add percent sign to value and delta\n    fig.update_traces(number={'suffix': '%'}, delta={'suffix': '%'})\n    # Visualize gauge in dark mode\n    fig.update_layout(template=\"plotly_dark\", font={'color': \"#00ff7f\", 'family': \"Arial\"}, height=500, width=364)\n    \n    return fig"
  },
  {
    "objectID": "projects/spotify_classification_dashboard/spotify_song_classification.html#panel-dashboard",
    "href": "projects/spotify_classification_dashboard/spotify_song_classification.html#panel-dashboard",
    "title": "Introduction",
    "section": "Panel Dashboard",
    "text": "Panel Dashboard\n\nimport panel as pn\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\npn.extension('echarts')\n\n# Create buttons for selecting models\nbutton_original_dataset = pn.widgets.Button(name = 'Original Dataset')\nbutton_logistic_regression = pn.widgets.Button(name='Logistic Regression')\nbutton_random_forest = pn.widgets.Button(name='Random Forest')\nbutton_boosted_trees = pn.widgets.Button(name='Boosted Trees')\nbutton_decision_trees = pn.widgets.Button(name='Decision Trees')\nbutton_svm = pn.widgets.Button(name='Support Vector Machine')\nbutton_knn = pn.widgets.Button(name='K-Nearest Neighbors')\n\nlast_reference = 0 # Create global variable to store the previous model's accuracy score\n\n# Define callback functions for the buttons\ndef on_click_original_dataset(event):\n    scatter_plot.object = create_original_scatter_plot(all_songs)\n    feature_importance_plot.object = corr_plot(all_songs) # Switch in a corr plot since there are no features to show\n\n    global last_reference\n    gauge_pane.object = gauge_accuracy_viz(0,0)\n\ndef on_click_logistic_regression(event):\n    scatter_plot.object = model_plot(y_pred_lr)  \n    feature_importance_plot.object = plot_linear_feature_importance(lr_model)\n    \n    global last_reference\n    gauge_pane.object = gauge_accuracy_viz(accuracy_lr, last_reference)\n    last_reference = accuracy_lr\n\ndef on_click_random_forest(event):\n    scatter_plot.object =  model_plot(y_pred_rf) \n    feature_importance_plot.object = plot_tree_feature_importance(rf_model)\n\n    global last_reference\n    gauge_pane.object = gauge_accuracy_viz(rf_accuracy, last_reference)\n    last_reference = rf_accuracy\n\ndef on_click_boosted_trees(event):\n    scatter_plot.object =  model_plot(y_pred_boost) \n    feature_importance_plot.object =  plot_tree_feature_importance(boost_model)\n\n    global last_reference\n    gauge_pane.object = gauge_accuracy_viz(boost_accuracy, last_reference)\n    last_reference = boost_accuracy\n\ndef on_click_decision_trees(event):\n    scatter_plot.object = model_plot(y_pred_dec_tree) \n    feature_importance_plot.object = plot_tree_feature_importance(dec_tree_model)\n    \n    global last_reference\n    gauge_pane.object = gauge_accuracy_viz(dec_tree_accuracy, last_reference)\n    last_reference = dec_tree_accuracy\n\ndef on_click_svm(event):\n    scatter_plot.object = model_plot(y_pred_svm) \n    feature_importance_plot.object = plot_linear_feature_importance(svm_model)\n\n    global last_reference\n    gauge_pane.object = gauge_accuracy_viz(svm_accuracy, last_reference)\n    last_reference = svm_accuracy\n\ndef on_click_knn(event):\n    scatter_plot.object = model_plot(y_pred_knn) \n    feature_importance_plot.object =  plot_tree_feature_importance(knn_model)\n\n    global last_reference\n    gauge_pane.object = gauge_accuracy_viz(knn_accuracy, last_reference)\n    last_reference = knn_accuracy\n\n\n# Bind callbacks when button is clicked\nbutton_original_dataset.on_click(on_click_original_dataset)\nbutton_logistic_regression.on_click(on_click_logistic_regression)\nbutton_random_forest.on_click(on_click_random_forest)\nbutton_boosted_trees.on_click(on_click_boosted_trees)\nbutton_decision_trees.on_click(on_click_decision_trees)\nbutton_svm.on_click(on_click_svm)\nbutton_knn.on_click(on_click_knn)\n\n\n# Create scatter plot widget\nscatter_plot = pn.pane.Plotly()\n\n# Create feature importance plot widget\nfeature_importance_plot = pn.pane.Plotly()  # plot_feature_importance(lr_model) \n\n# Create gauge visualization pane\ngauge_pane = pn.pane.Plotly() #gauge_accuracy_viz(rf_accuracy, last_reference)\n\n# Create logo pane\npanel_logo = pn.pane.PNG(\n    '/Users/piero/Downloads/Spotify_Project/Spotify_Logo_RGB_Green.png',\n    width=150, height=95, align='center'\n)\n\n#text1 = 'Visualize the performance of machine learning models in classifying songs from my playlist and my friends.' \ntext2 = 'Select a model using the buttons above to visualize its performance.' \ntext3 = '[View dashboard code](link_to_your_code)'\n\n# Dashboard layout\ntemplate = pn.template.FastListTemplate(theme=\"dark\",\n    logo = '/Users/piero/Downloads/Spotify_Project/Spotify_Logo_RGB_Green.png',\n    title = \"Visualizing Spotify Song Classification Performance\",\n    sidebar =[pn.pane.Markdown(\"## Reset\"),   \n             button_original_dataset, pn.pane.Markdown(\"## Models\"), button_logistic_regression, button_random_forest, \n             button_boosted_trees, button_decision_trees, button_svm, text2, text3],\n    main=[\n            pn.Row(pn.Column(scatter_plot, sizing_mode='stretch_both', margin=(-20,0,0,-24))),\n            pn.Row(pn.Column(feature_importance_plot, margin=(11,0,0,-24)),\n                   pn.Column(gauge_pane, margin=(11,0,0,-13)), sizing_mode='stretch_both', height=400, width=950\n                  )\n          ],\n    theme_toggle = False,\n    accent_base_color=\"#0bff38\", # change color of hyperlink text\n    header_background=\"#1f2630\", # change color of header banner | previous color: #009E60\n    header_color = '#0bff38', # change color of header text | previous color: #57ff76\n    main_max_width = '900',\n    main_layout = None, # maximum width of the main area containing all plots\n    sidebar_width=172, # adjust sidebar size\n    font = 'https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,100;1,100&display=swap'\n    \n) \n\n# Load original dataset button images on startup\non_click_original_dataset(None)\n\n# Display the dashboard\ntemplate.show()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWARNING:param.Row: Providing a width-responsive sizing_mode ('stretch_both') and a fixed width is not supported. Converting fixed width to min_width. If you intended the component to be fully width-responsive remove the heightsetting, otherwise change it to min_height. To error on the incorrect specification disable the config.layout_compatibility option.\nWARNING:param.Row: Providing a height-responsive sizing_mode ('stretch_both') and a fixed height is not supported. Converting fixed height to min_height. If you intended the component to be fully height-responsive remove the height setting, otherwise change it to min_height. To error on the incorrect specification disable the config.layout_compatibility option.\n/var/folders/th/h7c9tz61505fhg7ds00qjhlm0000gn/T/ipykernel_1168/3196471437.py:5: FutureWarning:\n\nThe default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n\n\nLaunching server at http://localhost:50831\n\n\n&lt;panel.io.server.Server at 0x12daf9bd0&gt;"
  },
  {
    "objectID": "projects/spotify_classification_dashboard/spotify_song_classification.html#final-thoughts",
    "href": "projects/spotify_classification_dashboard/spotify_song_classification.html#final-thoughts",
    "title": "Introduction",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nOverall, this project provided an amazing opportunity to delve into the realm of music data analysis, machine learning, and dashboarding. It was fascinating to uncover the intricate patterns within our Spotify Wrapped playlists and to conduct statistical comparisons of our music tastes. I was incredibly excited to visualize the similarities in our music tastes and gain deeper insights into our listening habits. While our classification models didn‚Äôt achieve perfection, they still yielded remarkably accurate results, hinting at meaningful distinctions in the songs favored by Nirvit and myself.\nFor those interested in conducting a similar analysis using Python, I recommend exploring my GitHub repository dedicated to this project."
  },
  {
    "objectID": "projects/spotify_classification_dashboard/spotify_song_classification.html#sources",
    "href": "projects/spotify_classification_dashboard/spotify_song_classification.html#sources",
    "title": "Introduction",
    "section": "Sources",
    "text": "Sources\nI‚Äôd like to extend a special thank you to the wonderful data analysts who inspired me to make this project, offering invaluable ideas and sharing fantastic source code. * Whose Song is it Anyway? By Lewis White. * How to Create a Beautiful Python Visualization Dashboard With Panel/Hvplot. By Thu Vu Data Analytics. * Predicting Song Popularity. By Alison Salerno. * App Gallery. By Panel."
  }
]